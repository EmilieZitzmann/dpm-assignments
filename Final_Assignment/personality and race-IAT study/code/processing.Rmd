---
title: "Examining the relationship between the big-5 personality facets and implicit racial attitudes"
subtitle: "Processing"
author: "Template: Ian Hussey, content: Emilie"
date: "2024-01-10"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r, include=FALSE}
#By this we set the knitr options (warnings and messages generated through R will not be shown in the final document)

knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)


```

# Dependencies

```{r}

library(tidyverse)
library(janitor) 
library(stringr)
library(openxlsx)
library(kableExtra)

```

# Get data

```{r}

# Demographics
data_raw_demographics <- read_csv("../data/raw/data_raw_demographics.csv") |>
  janitor::clean_names()

# Self report measure big five inventory 
data_raw_bfi <- read_csv("../data/raw/data_raw_bfi.csv") |>
  janitor::clean_names()

# Implicit association test
data_raw_iat <- read_csv("../data/raw/data_raw_iat.csv") |>
  janitor::clean_names()

```

# Demographics

Study participants had the option to provide their age and gender, although this was optional, meaning that missing data is not considered an exclusion criterion in this context. 

## Extraction of Age and Gender

Age and Gender should be extracted from the dataframe. As these are both within 'variable' two new variables named 'Age' and 'Gender' have to be created by using pivot_wider(). 

Prior to this, it is necessary to inspect the raw dataset for NA data within the 'unique_Id'. The resulting 'check_Id' dataframe reveals the presence of NA data, which is then removed from the new dataframe. In a subsequent step, missing values are isolated. Any values not meeting the specified criteria are assigned as 'other/missing/error' for potential exclusion at a later stage, if required.

```{r}

#Dataframe to check whether we have NA in unique_Id
check_Id <- data_raw_demographics |>
  select(unique_id, variable, response) |>
  mutate(exclude_Id= ifelse (is.na(unique_id), 
                             "missing", 
                             "inlcude")) |>
  group_by(unique_id) 

#Creating a new dataset where we exclude the NA and the duplicated rows. Be careful not to just exclude the 'missing' ones from above as this would make both unique_id values disappear. 

data_age_gender <- data_raw_demographics |>
  select(unique_id, variable, response) |>
  filter(!is.na(unique_id)) |> #filter out the NA 
  pivot_wider(names_from = variable,
              values_from = response) |> 
  rename(gender = sex) |>
  mutate(gender = case_when(gender %in% c("f", "m") ~ gender, #returns current value for gender if TRUE
                             is.na(gender) ~ "other/missing/error", #if gender equals NA
                             TRUE ~ "other/missing/error"), #if none of the set conditions is true 
         age = case_when(str_detect(age, "^[0-9]+$") ~ age, #setting the wanted value for age = number
                         is.na(age) ~ "other/missing/error",#if age equals NA
                         TRUE ~ "other/missing/error")) 

```

# BFI 

As part of the study, each respondent was asked to complete between two and three subscales of the Big 5 personality inventory.

A check is first carried out to determine whether there are duplicate lines in the bfi_dataset. As no duplicates could be detected, the raw bfi dataset is worked with directly in the following steps.

```{r}

check_bfi <-  data_raw_bfi |> 
  distinct() 

```

## Reverse Score negatively worded items

Specific items were worded negatively. These items need to be reversed in order to be suitable for analysis. This applies to the items: 

-	the extroversion scale items: bfi_e2, bfi_e5, bfi_e7
-	conscientiousness items: bfi_c2, bfi_c4, bfi_c5, bfi_c9
-	neuroticism items: bfi_n2, bfi_n5, bfi_n7
-	agreeableness items: bfi_a1, bfi_a3, bfi_a6, bfi_a8
-	openness items: bfi_o7, bfi_o9 
 
Each value can reach a maximum of 6 and a minimum of 1. In order to return the Items we must remember that the reverted scale is behaving inversely to the positive one so 1 = 6, 2 =5, 3=4, 4=3, 2= 5, 6=1. 
The easiest way to revert a scale is to add 1 to the maximum value in order to subtract the original value from it. This way we get the reversed score value. 

To modify the data the mutate() function is used, to apply the function on multiple columns e.g. the produced vector we can use across(). 

```{r}

vector_reversed_items <- c("bfi_e2","bfi_e5","bfi_e7","bfi_c2","bfi_c4","bfi_c5","bfi_c9","bfi_n2","bfi_n5", "bfi_n7","bfi_a1", "bfi_a3","bfi_a6","bfi_a8","bfi_o7","bfi_o9")


data_reversed_items <- data_raw_bfi |>
  mutate(across(all_of(vector_reversed_items), ~7 - .)) #'.' represents column values that are subtracted                                                            from 7 (by performing ~7). This produces the                                                                reverted items. 
 
```


### Sanity Check 

To check whether the reversals are likely to be correct a correlation matrix for each variable is           created. If the correlation is negative or rather low it could indicate that the reversing was not          carried out properly. As the Big Five tend to be normally distributed the Pearson correlation is used to calculate the matrix. The 'use' within the correlation function is set to complete.obs, which means that    variables that contain missing data are not included. This is important considering that the participants  only filled out two or three of the subscales which is producing some NA for each row, hindering to         calculate the correlation correctly. 

In order to make sure that the newly created correlation matrix with the reverted items produces less/no    negative results, a matrix is calculated for the original dataset and compared with the matrix of the       reversed one. 

```{r}

#Extroversion
Matrix_extroversion_reversed <- data_reversed_items |>
  select(contains("bfi_e")) |>
  cor(method = "pearson", use = "complete.obs")

Matrix_extroversion_original <- data_raw_bfi |>
   select(contains("bfi_e")) |>
  cor(method = "pearson", use = "complete.obs")


#Conscientiousness 
Matrix_conscient_reversed <- data_reversed_items |>
  select(contains("bfi_c")) |>
  cor(method = "pearson", use = "complete.obs")

Matrix_conscient_original <- data_raw_bfi |>
   select(contains("bfi_c")) |>
  cor(method = "pearson", use = "complete.obs")


#Neuroticism
Matrix_neuroticism_reversed <- data_reversed_items |>
  select(contains("bfi_n")) |>
  cor(method = "pearson", use = "complete.obs")

Matrix_neuroticism_original <- data_raw_bfi |>
   select(contains("bfi_n")) |>
  cor(method = "pearson", use = "complete.obs")


#Agreeableness
Matrix_agreeable_reversed <- data_reversed_items |>
  select(contains("bfi_a")) |>
  cor(method = "pearson", use = "complete.obs")

Matrix_agreeable_original <- data_raw_bfi |>
   select(contains("bfi_a")) |>
  cor(method = "pearson", use = "complete.obs")


#Openness
Matrix_openness_reversed <- data_reversed_items |>
  select(contains("bfi_o")) |>
  cor(method = "pearson", use = "complete.obs")

Matrix_openness_original <- data_raw_bfi |>
   select(contains("bfi_o")) |>
  cor(method = "pearson", use = "complete.obs")


```

*Interpretation*: The reversal procedure appears to be effective, as the only remaining negative            correlation can be observed between items bfi_o7 and bfi_o10. In comparison to the original matrix, there   is a substantial reduction in the overall negative correlations. 

A second sanity check is that the logical minimum (1) and maximum (6) are not violated. In order to check   that an exclusion variable is created that sets participants with impossible data to 'exclude'. Again       specific information is selected with c_across()

```{r}

#Defining the vector for maximum and minimum 

logical_min <- 1 
logical_max <- 6

#creating the dataset 
data_logical_number <- data_reversed_items |>
  rowwise() |>
  mutate(bfi_logical_number_violation = ifelse(any(!is.na(c_across(contains("bfi"))) & #selected values are not NA
                                     (c_across(contains("bfi")) < logical_min | 
                                      c_across(contains("bfi")) > logical_max)),
                                 "exclude",
                                 "include")) |>
  ungroup() |> #The ungroup() argument is used after rowwise() in order to prevent later calculations being applied rowwise as well
  select(unique_id, bfi_logical_number_violation)

```

*Interpretation*: To check whether individuals violate the logical minimum or maximum, they can be          identified through filtering. This concerns 3 individuals. The dataset illustrates that they all included   7 in one response, which justifies the exclusion. 

```{r}

data_logical_number |>
  filter(bfi_logical_number_violation == "exclude") |>
  select(unique_id) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

To perform a third quality check of the BFI data, we ensure that all participants have completed data for   the surveys they filled out. Initially, a new variable is created for each survey and assigned 'include'    if all values in the specified row of the survey are present and not missing. We then filter out the        'excluded' observations based on that criterion. After summarizing this information in a new data frame,    the dataset is further refined by excluding people who completed less than two questionnaires. This step    aims to minimize unnecessary exclusions, taking into account that individuals with missing information      may still have completed at least 2 questionnaires. In the final step, this cleaned data frame is merged    with the reversed data frame and all newly added rows are removed, resulting in a refined and cleaned       dataset.

```{r}

#Extroversion
 data_extroversion_exclusion <- data_reversed_items |> 
   rowwise() |> 
   mutate(completed_q_e = ifelse(all(!is.na(c_across(contains("bfi_e")))), "include", "exclude")) |>
   filter(completed_q_e != "exclude") 
 
#Conscientiousness
data_conscient_exclusion <- data_reversed_items |> 
  rowwise() |> 
  mutate(completed_q_c = ifelse(all(!is.na(c_across(contains("bfi_c")))), "include", "exclude")) |> 
  filter(completed_q_c != "exclude")
 

#Neuroticism
data_neuroticism_exclusion <- data_reversed_items |> 
  rowwise() |> 
  mutate(completed_q_n = ifelse(all(!is.na(c_across(contains("bfi_n")))), "include", "exclude")) |>
  filter(completed_q_n != "exclude")

#Agreeableness
data_agreeable_exclusion <- data_reversed_items |> 
  rowwise() |> 
  mutate(completed_q_a = ifelse(all(!is.na(c_across(contains("bfi_a")))), "include", "exclude")) |>
  filter(completed_q_a != "exclude")

#Openness
data_openness_exclusion <- data_reversed_items |> 
  rowwise() |> 
  mutate(completed_q_o = ifelse(all(!is.na(c_across(contains("bfi_o")))), "include", "exclude")) |>
  filter(completed_q_o != "exclude")



#combining all of the information above within one dataframe and checking whether each person has at least completed 2 Bfi Scales 

data_combined_exclusion <- data_reversed_items |>
  select(unique_id) |>
  left_join(data_extroversion_exclusion |>
            select(unique_id, completed_q_e), by = "unique_id") |>
  left_join(data_conscient_exclusion |>
            select(unique_id, completed_q_c), by = "unique_id") |>
  left_join(data_neuroticism_exclusion |>
            select(unique_id, completed_q_n), by = "unique_id") |>
  left_join(data_agreeable_exclusion |>
            select(unique_id, completed_q_a), by = "unique_id") |>
  left_join(data_openness_exclusion |>
            select(unique_id, completed_q_o), by = "unique_id") |>
  rowwise() |>
  mutate(completed_questionnaires = sum(!is.na(c(completed_q_e, 
                                                 completed_q_c, 
                                                 completed_q_n,
                                                 completed_q_a,
                                                 completed_q_o))),
         bfi_questionnaire_inclusion = case_when(completed_questionnaires >= 2 ~ 
                                             "include",
                                             TRUE ~ "exclude"))


#Making a final dataframe 

data_completed_exclusion <- data_reversed_items |>
  left_join(data_combined_exclusion, by = "unique_id") |>
  select(unique_id, bfi_questionnaire_inclusion) 

```

  *Interpretation*: Again, the individuals in question are selected using filter. It turns out that seven      respondents omitted an answer in their questionnaires and therefore completed fewer than 2 of them. This    is evident by checking with the dataset. 

```{r}

data_completed_exclusion |>
  filter(bfi_questionnaire_inclusion == "exclude") |>
  select(unique_id) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

## Mean score BFI

The mean values corresponding to the subscales in the Big Five Inventory for subsequent analysis are calculated. To ensure the integrity of the data, it was made sure that the mean values corresponded to the defined minimum and maximum values. 

```{r}

data_mean_bfi <- data_reversed_items |> 
  rowwise() |>
  mutate(mean_extroversion = mean(c_across(contains("bfi_e")), na.rm =T),
         mean_conscient = mean(c_across(contains("bfi_c")), na.rm = T),
         mean_neuroticism = mean(c_across(contains("bfi_n")), na.rm = T),
         mean_agreeable = mean(c_across(contains("bfi_a")), na.rm =T),
         mean_openness = mean(c_across(contains("bfi_o")), na.rm =T))


#Check for violation of the scores, do not take the same vector name as above because if there is some change done in either of them the calculation will get mixed up.

min_possible_score <- 1  
max_possible_score <- 6  


data_mean_violation_bfi <- data_mean_bfi |>
  rowwise() |>
  mutate(bfi_mean_violation = ifelse(all(c_across(contains("bfi")) #use all so it is checked for each row
                                      < min_possible_score | c_across(contains("bfi")) 
                                      >max_possible_score, na.rm = T),
                                  "exclude", 
                                  "include")) |>
  ungroup() |>
  select(unique_id, mean_extroversion, mean_conscient, mean_neuroticism, mean_agreeable, mean_openness,
         bfi_mean_violation)

```

*Interpretation*: There is no individual mean score violating the logical minimum or maximum. 

```{r}
 
data_mean_violation_bfi |>
  filter(bfi_mean_violation == "exclude") |>
  select(unique_id) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

# IAT 

Within the study each participant completed an Implicit Association Test assessing implicit racial evaluations between African-American and European-American people. Block 1, 2, 5 are practice blocks and must be discarded for analysis calculations. 

An initial visual inspection of the dataset revealed that certain variables were only of limited use, therefore these were first adjusted. It was then ensured that 'trial_reaction_time' remained a numeric variable. The removal of character labels (the trial reaction time in ms previously located there) within the numerical variable can lead to conversion problems. The same is done fore 'unique_Id' as this has been numerical for the other datasets. 

```{r}

data_iat_corrected <- data_raw_iat |>
  rename(unique_id = participant, #rename the variables adequately 
         block_number = block,
         block_required_responses = x3,
         trial_responses = trial,
         trial_accuracy = x5,
         trial_reaction_time_in_ms = x6) |>
  tail(-1) |>
  mutate(unique_id = as.numeric(unique_id), #Making sure trial reaction and unique id are numeric 
         trial_reaction_time_in_ms = as.numeric(trial_reaction_time_in_ms))

```

## Scoring the trial-level data 

First, the average response time ('mean1') is determined for the trials in blocks 3 and 6. Next, another mean response time ('mean2') is computed for the trials in blocks 4 and 7. The standard deviation of the response times in blocks 3, 4, 6 and 7 ('SD') and the Greenwald score 'D' are calculated. 

```{r}

#Calculation of mean Reaction Time block 3 and 6: mean1

mean1 <- data_iat_corrected |>
  filter(block_number %in% c(3, 6)) |> #select relevant rows 
  group_by(unique_id) |> #group the data by the unique_id
  summarize(mean1 = mean(trial_reaction_time_in_ms, na.rm = TRUE)) #calculating mean RT for unique_id


#Calculation of mean Reaction Time block 4 and 7: mean2

mean2 <- data_iat_corrected |>
  filter(block_number %in% c(4, 7)) |>  
  group_by(unique_id) |> 
  summarise(mean2 = mean(trial_reaction_time_in_ms, na.rm = TRUE)) 


#Calculation of SD Reaction Time in block 3, 4, 6, 7: 

SD <- data_iat_corrected |>
  filter(block_number %in% c(3, 4, 6, 7)) |>  
  group_by(unique_id) |> 
  summarise(SD = sd(trial_reaction_time_in_ms, na.rm = TRUE)) 


#D is calculated through (mean2 - mean1)/SD. 

D <- left_join(mean2, mean1, by = "unique_id") |> # we need mean1 and 2 in order to calculate
  left_join(SD, by = "unique_id") |> #sd is also required for calculation
  mutate(D = (mean2 - mean1) / SD) |> #creating the new variable
  select(unique_id, D)


#Final dataframe with all scores 
data_iat_score <- D |>
  left_join(mean1, by = "unique_id") |>
  left_join(mean2, by = "unique_id") |>
  left_join(SD, by = "unique_id") 
```

### Sanity Check

A first sanity check verifies that each D score falls within the specified logical range of -2 to +2

```{r}

#Define the min and max variable, again do not take the same ones as above

min_range_d <- -2  
max_range_d <- +2 

data_d_violation <- data_iat_score |>
  mutate(d_score_range = ifelse(D < min_range_d | D > max_range_d, 
                                "exclude", 
                                "include")) |>
  select(unique_id, d_score_range)
```

  *interpretation*: This data set does not contain any violations of the logical minimum and maximum within    the D-score. 

```{r}

data_d_violation |>
  filter(d_score_range == "exclude") |>
  select(unique_id) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

## Exclusion Variable for incomplete IAT data

An exclusion variable is created that sets participants with incomplete IAT data at trial level to        'exclude'. The IAT should contain 120 trials in the relevant test blocks required for the analysis.

```{r}

data_iat_incomplete <- data_iat_corrected |>
  group_by(unique_id) |>
  mutate(trial_count = sum(block_number %in% c(3, 4, 6, 7))) |>
  ungroup() |>
  mutate(iat_completed_trials = ifelse(trial_count == 120, "include", "exclude")) |>
  select(unique_id, iat_completed_trials) |>
  distinct()  # Only keep one row per ID as they have multiple rows due to several trials 

```

*Interpretation* : Some individuals have uncompleted trial data and must therefore be set to 'exclude'.

```{r}

data_iat_incomplete |>
  filter(iat_completed_trials == "exclude") |>
  select(unique_id) |>
  kable() |>
  kable_classic(full_width = FALSE)
```

## Exclusion Variable for IAT performance

Participants who have over 10% of their trials with reaction times less than 300                            milliseconds or an accuracy below 75% should be excluded. In order to do so, two variables that contain     the corresponding exclusion criteria are created. A proportion of these is then calculated so that          individuals can be identified for exclusion. 

```{r}

data_iat_exclusion <- data_iat_corrected |>
  filter(block_number %in% c(3, 4, 6, 7)) |> #Selecting the blocks 
  mutate(low_reaction_time = ifelse(trial_reaction_time_in_ms < 300, TRUE, FALSE), #low reaction time 
         low_accuracy = ifelse(trial_accuracy == "incorrect", TRUE, FALSE)) |> #T only if incorrect
  group_by(unique_id) |>
  summarize(proportion_fast_reaction_time = mean(low_reaction_time), #calculate the proportions 
            proportion_low_accuracy_iat = mean(low_accuracy)) |> 
  mutate(exclude_accuracy = ifelse(proportion_fast_reaction_time >= 0.10 | # 10 %
                                   proportion_low_accuracy_iat >= 0.75, # 75 %
                                   "exclude",
                                   "include"))
```

*Interpretation*: One person has to be excluded due to two main reasons. First, the accuracy is only         38.33%, which indicates a high proportion of 'incorrect' trials. Secondly, about 46.67% of the trials       have a reaction time of less than 300 ms, which also exceeds the acceptable threshold. Thus, this person    should be excluded from further analysis.

```{r}
data_iat_exclusion|>
  filter(exclude_accuracy == "exclude") |>
  select(unique_id) |>
  kable() |>
  kable_classic(full_width = FALSE)
```

# Combine

The datasets are to be merged into a single dataframe, ensuring each participant is represented by a single row. The datasets to combine are:

-	data_age_gender
-	data_reversed_items
-	data_logical_number
-	data_completed exclusion
-	data_mean_violation_bfi
-	data_iat_score
-	data_d_violation
-	data_iat_incomplete
-	data_iat_exclusion

```{r}

data_processed_all <- data_age_gender |>
  full_join(data_reversed_items, by = "unique_id") |>
  full_join(data_logical_number, by ="unique_id") |>
  full_join(data_completed_exclusion, by= "unique_id") |>
  full_join(data_mean_violation_bfi, by = "unique_id") |>
  full_join(data_iat_score, by = "unique_id") |>
  full_join(data_d_violation, by = "unique_id") |> #It contains no exclude but for the sake of completeness
  full_join(data_iat_incomplete, by = "unique_id") |>
  full_join(data_iat_exclusion, by = "unique_id")
  
  
# flag all subjects with more than one row in the wide-format data. these should be excluded in the analysis.

data_processed_duplicates <- data_processed_all |>
  count(unique_id) |>
  mutate(exclude_duplicate_data = if_else(n > 1, "exclude", "include")) |>
  select(-n)

# join in the duplicates df
data_processed_before_exclusions <- data_processed_all |>
  full_join(data_processed_duplicates, by = "unique_id")

```

# Define master exclusions

```{r}

# create a master exclude_participant variable

data_processed <- data_processed_before_exclusions |>
  mutate(exclude_participant = case_when(is.na(bfi_logical_number_violation) |
                                         is.na(bfi_questionnaire_inclusion) |
                                         is.na(bfi_mean_violation) |
                                         is.na(d_score_range) |
                                         is.na(iat_completed_trials) |
                                         is.na(exclude_accuracy)
                                         ~ "exclude",
                                         bfi_logical_number_violation == "exclude" ~ "exclude",
                                         bfi_questionnaire_inclusion == "exclude" ~ "exclude",
                                         bfi_mean_violation == "exclude" ~ "exclude",
                                         d_score_range == "exclude" ~ "exclude",
                                         iat_completed_trials == "exclude" ~ "exclude",
                                         exclude_accuracy == "exclude" ~ "exclude",
                                         TRUE ~ "include")) 

#Check: how many participants got filtered out? 
filtered_count <- data_processed |>
  group_by(exclude_participant) |>
  summarize(count = n())

# Display the result
print(filtered_count)


```

*Interpretation*: Given 179 participants in the original dataset, where each unique ID is only listed once,  the result of 164 participants is reasonable. We excluded 15 participants by means of the analyses listed   above.

# Write to disk

```{r}

# in case this dir doesn't exist it is created
dir.create("../data/processed/")

# save data to disk in that dir
write_csv(data_processed, "../data/processed/data_processed.csv")

```

# Codebook for the processed data

```{r}

if(!file.exists("../data/processed/data_processed_codebook.xlsx")){ # convert the column names to a df
  codebook_template <- data.frame(variable = colnames(data_processed)) |>
    mutate(explanation = NA) 
  write.xlsx(codebook_template, 
             file = "../data/processed/data_processed_codebook.xlsx")} # write to disk as an excel file

```

# Session info

```{r}

sessionInfo()

```